{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11b75864",
   "metadata": {},
   "source": [
    "# Project - experimentation\n",
    "\n",
    "This is used for experimenting with ollama running in the on-prem data center on ulysses.local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea449eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "548d7b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n"
     ]
    }
   ],
   "source": [
    "# Initialization\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "# MODEL = \"gpt-4.1-mini\"\n",
    "# openai = OpenAI()\n",
    "\n",
    "# As an alternative, if you'd like to use Ollama instead of OpenAI\n",
    "# Check that Ollama is running for you locally (see week1/day2 exercise) then uncomment these next 2 lines\n",
    "# MODEL = \"llama3.2\"                 # Best lightweight general chat\n",
    "# MODEL = \"llama3.2:latest\"          # Best small all-rounder (default)\n",
    "# MODEL = \"llama3.1:8b\"              # Solid general reasoning, older gen\n",
    "# MODEL = \"deepseek-r1:8b\"           # Fast reasoning + logic experiments\n",
    "# MODEL = \"deepseek-r1:14b\"          # Smartest local reasoning model\n",
    "# MODEL = \"deepseek-coder-v2:16b\"    # Strong logic-heavy coding\n",
    "MODEL = \"deepseek-coder-v2:latest\" # Best for complex debugging + reasoning\n",
    "# MODEL = \"llama2:13b\"               # Legacy large model, slower\n",
    "# MODEL = \"llama3.2:3b\"              # Fast lightweight assistant\n",
    "# MODEL = \"llama3:8b\"                # Balanced chat + reasoning\n",
    "# MODEL = \"llama3.2:1b\"              # Ultra-fast low-quality tests\n",
    "# MODEL = \"qwen2.5-coder:7b\"         # Fast coding iteration\n",
    "# MODEL = \"qwen2.5-coder:14b\"        # Best overall coding model\n",
    "\n",
    "\n",
    "openai = OpenAI(base_url='http://ulysses.local:11434/v1', api_key='ollama')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa1b1934",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "You are an expert on writing code in assembly for the 6510 microprocessor, targetting the commodore 64 computer.\n",
    "You are also an expert on the Commodore 64 computer and its peripherals.\n",
    "You have to ensure that the code is correct and will run on the Commodore 64 computer.\n",
    "You have to ensure that code is properly de-composed into sub-routines and functions and that they are named appropriately.\n",
    "You have to ensure that the code is well-documented and that it is easy to understand.\n",
    "C64 is synonomous with the Commodore 64 computer.\n",
    "When assembly code or a program is requested, we are talking about assembly language code for the 6510 microprocessor to run on the Commodore 64 computer.\n",
    "Programs should always run with the Commodore 64 BASIC interpreter, so you don't have to worry about that. So load \"*\",8,1 should work etc.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3cc48e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7881\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7881/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chat(message, history):\n",
    "    history = [{\"role\":h[\"role\"], \"content\":h[\"content\"]} for h in history]\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=MODEL, messages=messages)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "794bf54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread 1: starting\n",
      "Thread 1 : 10\n",
      "Thread 2: starting\n",
      "Thread 2 : 10\n",
      "Thread 1 : 9\n",
      "Thread 2 : 9\n",
      "Thread 1 : 8\n",
      "Thread 2 : 8\n",
      "Thread 1 : 7\n",
      "Thread 2 : 7\n",
      "Thread 1 : 6\n",
      "Thread 2 : 6\n",
      "Thread 2 : 5\n",
      "Thread 1 : 5\n",
      "Thread 2 : 4\n",
      "Thread 1 : 4\n",
      "Thread 2 : 3\n",
      "Thread 1 : 3\n",
      "Thread 2 : 2\n",
      "Thread 1 : 2\n",
      "Thread 2 : 1\n",
      "Thread 1 : 1\n",
      "Thread 1: endingThread 2: ending\n",
      "\n",
      "All threads have finished\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import time\n",
    "\n",
    "def thread_func(name):\n",
    "    print(f\"Thread {name}: starting\")\n",
    "    for i in range(10,0,-1): # Count down from 10 to 1\n",
    "        print(f\"Thread {name} : {i}\")\n",
    "        time.sleep(1)\n",
    "    print(f\"Thread {name}: ending\")\n",
    "\n",
    "thread_1 = threading.Thread(target=thread_func, args=(1,))\n",
    "thread_2 = threading.Thread(target=thread_func, args=(2,))\n",
    "\n",
    "# start the thread\n",
    "thread_1.start()\n",
    "thread_2.start()\n",
    "\n",
    "# wait for threads to finish\n",
    "thread_1.join()\n",
    "thread_2.join()\n",
    "\n",
    "print(\"All threads have finished\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
